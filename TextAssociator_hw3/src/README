Tom Nguyen
01/31/17
CSE 373
TA: Raquel Van Hofwegen
HW #3

1) address everyone the genuine article is jive in contemplation of paint regulation and allege fruition hereby apriorism structures

2) I chose to create my starting capacity of my TextAssociator to be a prime number of 53. We learned that keeping the table size a prime number made the table less susceptible to collisions because you have to mod the hash code by the table size. Also, I did not want to start the table at a small prime number because then it would resize too early on. 

I chose to resize the hash table once the load factor became 1.00. From lecture, we learned that in general the higher the load factor, the more your runtime efficiency of insertions into the hash table will degrade. We also learned that it is best for separate chaining's load factor should ideally be between 1.00 and 2.00. Although separate chaining could exceed a load factor of 1.00, its tradeoff was that it requires more memory/space to store itself. However, in my opinion, memory is not as vital as speed so I sacrificed memory and capped my resizing threshold to be at 1.00 so that the find()/delete()/add() functions would not degrade their runtime too much. One drawback may be that if somehow all words/associations all miraculously hashed to one bucket. Maybe it would be better to resize when a ratio of words per bucket exceeded a certain ratio relative to the table size.

Once resized, the new size of my table was the next index of the array of primes that were each about twice as large as the previous index before itself. In class, we learned that it is best to keep the table size a prime number. Therefore, doubling the table size would lose this property because it will obviously be divisible by 2 and the previous size. This would mean less uniqueness for placement in our hash tables because you would be modding by a less unique number.

3) I decided to use the WordInfo's implemented hashCode(). The reason may have been pedantic though. I looked at the implemented hashCode() in the WordInfo Class file and from my understanding, it incoporates the String Class' hashCode() and multiplies it by 31 (a prime number), increasing the uniqueness. On top of this, I made it so that my table size was a prime number. Because the position of the element being inserted is modded by the table size, this further increased the unique positioning in the hash table for each inserted element. Also, the WordInfo hashCode() returned the absolute value of a number. This restricts the hash code from being a negative number, which made for less work when placing in the hash table because there aren't negative indicies in an array. After some thought, the motivation behind incorporating a prime number into a hash function is to increase uniqueness, but the hash function the String Class already multiplies its value by a prime number. Using some math knowledge and some research on cryptography/encryption, on top of using the String hashCode(), the WordInfo hashCode() multiplies it by another prime number. This somewhat creates a semiprime (prime multiplied by another prime but I know that the String hashCode() also multiplies by the char ASCII values) and semiprimes are highly regarded in cryptography for being good numbers for creating codes that are hard to decipher, implying that there is a certain higher level of uniqueness to the number. So it does seem like the WordInfo hashCode() is implemented well. 

4) I would choose to use double hashing. Unfortunately, linear probing would not work too well because there are many words in the English language (or any language that can be used with the TextAssociator) that have the same letters in them but are just in a different order, ie. dog or God. This is bound to create many collisions and lots of collisions in linear probing causes primary clustering, which degrades runtime efficiency. Quadratic probing would be worse in my opinion because it could cause infinite looping when trying to add something to the hash table. With double hashing, the TextAssociator would not need the WordInfoSeparateChain to be stored as a list. Actually, it would not need the WordInfoSeparateChain class at all. Instead it would just place the WordInfo would double hash to. Additionally, I would have to create another hash function to use for collision resolution. Also, the threshold of the load factor would be decreased because it should not be close to 1.00 because double hashing cannot exceed 1.00 load factor. 